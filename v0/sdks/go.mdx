---
title: 'Go'
description: 'Firecrawl Go SDK 是一个包装在 Firecrawl API 上的库，帮助您轻松将网站转换为 markdown。'
icon: 'golang'
og:title: "Go SDK | Firecrawl"
og:description: "Firecrawl Go SDK 是一个包装在 Firecrawl API 上的库，帮助您轻松将网站转换为 markdown。"
---

> 注意：此文档使用的是 [Firecrawl API 的 v0 版本](/v0/introduction)，该版本已被弃用。我们建议切换到 [v1](/sdks/go)。

## 安装

要安装 Firecrawl Go SDK，您可以使用 go get：

```bash
go get github.com/mendableai/firecrawl-go
```

## 使用

1. 从 [firecrawl.dev](https://firecrawl.dev) 获取一个 API key
2. 将 API key 设置为环境变量 `FIRECRAWL_API_KEY` 或将其作为参数传递给 `FirecrawlApp` 结构体。


以下是如何使用 SDK 并处理错误的示例：

```go
import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  // 使用您的 API key 初始化 FirecrawlApp
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("无法初始化 FirecrawlApp: %v", err)
  }

  // 抓取单个 URL
  scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("抓取时发生错误: %v", err)
  }
  fmt.Println(scrapedData)

  // 爬取一个网站
  params := map[string]any{
    "pageOptions": map[string]any{
      "onlyMainContent": true,
    },
  }

  crawlResult, err := app.CrawlURL("docs.firecrawl.dev", params)
  if err != nil {
    log.Fatalf("爬取时发生错误: %v", err)
  }
  fmt.Println(crawlResult)
}
```


### 抓取一个 URL

使用 `ScrapeURL` 方法抓取单个 URL 并处理错误。它接受 URL 作为参数并返回抓取的数据作为字典。

```go
scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
if err != nil {
  log.Fatalf("无法抓取 URL: %v", err)
}
fmt.Println(scrapedData)
```


### 爬取一个网站

使用 `CrawlUrl` 方法爬取一个网站。它接受起始 URL 和可选参数。`params` 参数允许您指定额外的选项，如最大页面数量、允许的域和输出格式。

```go
crawlParams := map[string]any{
  "crawlerOptions": map[string]any{
    "excludes": []string{"blog/*"},
    "includes": []string{}, // 留空表示所有页面
    "limit": 1000,
  },
  "pageOptions": map[string]any{
    "onlyMainContent": true,
  },
}
crawlResult, err := app.CrawlURL("docs.firecrawl.dev", crawlParams, true, 2, idempotencyKey)
if err != nil {
  log.Fatalf("无法爬取 URL: %v", err)
}
fmt.Println(crawlResult)
```


### 检查爬取状态

使用 `CheckCrawlStatus` 方法检查爬取作业的状态。它接受作业 ID 作为参数并返回当前爬取作业的状态。

```go
status, err := app.CheckCrawlStatus(jobId)
if err != nil {
  log.Fatalf("无法检查爬取状态: %v", err)
}
fmt.Println(status)
```


### 取消爬取作业

使用 `CancelCrawlJob` 方法取消爬取作业。它接受作业 ID 作为参数并返回取消状态。

```go
canceled, err := app.CancelCrawlJob(jobId)
if err != nil {
  log.Fatalf("无法取消爬取作业: %v", err)
}
fmt.Println(canceled)
```


### 从 URL 提取结构化数据

使用 LLM 提取，可以轻松地从任何 URL 提取结构化数据。以下是如何使用它的示例：

```go
jsonSchema := map[string]any{
  "type": "object",
  "properties": map[string]any{
    "top": map[string]any{
      "type": "array",
      "items": map[string]any{
        "type": "object",
        "properties": map[string]any{
          "title":       map[string]string{"type": "string"},
          "points":      map[string]string{"type": "number"},
          "by":          map[string]string{"type": "string"},
          "commentsURL": map[string]string{"type": "string"},
        },
        "required": []string{"title", "points", "by", "commentsURL"},
      },
      "minItems":    5,
      "maxItems":    5,
      "description": "Hacker News 上的前 5 个故事",
    },
  },
  "required": []string{"top"},
}

llmExtractionParams := map[string]any{
  "extractorOptions": firecrawl.ExtractorOptions{
    ExtractionSchema: jsonSchema,
  },
}

scrapeResult, err := app.ScrapeURL("https://news.ycombinator.com", llmExtractionParams)
if err != nil {
  log.Fatalf("无法执行 LLM 提取: %v", err)
}
fmt.Println(scrapeResult)
```


### 搜索查询

要搜索网络，获取最相关的结果，抓取每个页面并返回 markdown，请使用 `Search` 方法。该方法接受查询作为参数并返回搜索结果。


```go
query := "What is firecrawl?"
searchResult, err := app.Search(query)
if err != nil {
  log.Fatalf("搜索失败: %v", err)
}
fmt.Println(searchResult)
```


## 错误处理

SDK 处理由 Firecrawl API 返回的错误并引发适当的异常。如果请求过程中发生错误，将会引发带有描述性错误消息的异常。